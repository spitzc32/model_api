{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://dtn.jfrog.io/artifactory/api/pypi/pypi/simple\n",
      "Collecting genpipes\n",
      "  Using cached https://dtn.jfrog.io/artifactory/api/pypi/pypi/packages/packages/a6/25/da9dc4f8ff0e38e75edbbef43aff2e64c972746778890883f83543c441ba/genpipes-1.0.0-py3-none-any.whl (4.8 kB)\n",
      "Installing collected packages: genpipes\n",
      "Successfully installed genpipes-1.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://dtn.jfrog.io/artifactory/api/pypi/pypi/simple\n",
      "Collecting nltk\n",
      "  Downloading https://dtn.jfrog.io/artifactory/api/pypi/pypi/packages/packages/43/0b/8298798bc5a9a007b7cae3f846a3d9a325953e0f9c238affa478b4d59324/nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m39.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading https://dtn.jfrog.io/artifactory/api/pypi/pypi/packages/packages/91/d4/3b4c8e5a30604df4c7518c562d4bf0502f2fa29221459226e140cf846512/joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m34.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Using cached https://dtn.jfrog.io/artifactory/api/pypi/pypi/packages/packages/47/bb/849011636c4da2e44f1253cd927cfb20ada4374d8b3a4e425416e84900cc/tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading https://dtn.jfrog.io/artifactory/api/pypi/pypi/packages/packages/84/93/67595e62890fa944da394795f0425140917340d35d9cfd49672a8dc48c1a/regex-2022.10.31-cp310-cp310-macosx_10_9_x86_64.whl (293 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.9/293.9 kB\u001b[0m \u001b[31m32.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Installing collected packages: tqdm, regex, joblib, nltk\n",
      "Successfully installed joblib-1.2.0 nltk-3.7 regex-2022.10.31 tqdm-4.64.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install genpipes\n",
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  \n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jayragaileortiz/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "tags = [\"I-NAME\", \"I-PROFESSION\", \"I-LOCATION\", \"I-AGE\", \"I-DATE\", \"I-CONTACT\", \"I-ID\", \"I-PHI\",  \"O\"]\n",
    "nltk.download('punkt')\n",
    "\n",
    "class Tags():\n",
    "    def __init__(self):\n",
    "        self.entities = []\n",
    "        self.tags = []\n",
    "        self.sub_tags = [] \n",
    "    \n",
    "    def append_entity(self, entity: list):\n",
    "        self.entities.extend(entity)\n",
    "    \n",
    "    def append_tag(self, tag: list):\n",
    "        self.tags.extend(tag)\n",
    "    \n",
    "    def append_sub_tags(self, sub_tag: list):\n",
    "        self.sub_tags.extend(sub_tag)\n",
    "    \n",
    "\n",
    "def gather_tags(root):\n",
    "    entities = []\n",
    "    tags = []\n",
    "    sub_tags = []\n",
    "    for child in root.iter('TAGS'):\n",
    "        for subchild in child:\n",
    "            entity = subchild.get(\"text\").split(\" \")\n",
    "            [tags.append(subchild.tag) for ent in entity]\n",
    "            [sub_tags.append(subchild.get(\"TYPE\")) for ent in entity]\n",
    "            entities.extend(entity)\n",
    "    \n",
    "    tags.append(\"O\")\n",
    "    return entities, tags, sub_tags\n",
    "\n",
    "def split_to_sent(root):\n",
    "    doc_txt = str(root.find('TEXT').text)\n",
    "    tokens = sent_tokenize(doc_txt)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def split_to_tokens(text):\n",
    "    tokens = word_tokenize(text, preserve_line=True)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def label_txt(root):\n",
    "    new_tags = [\"I-NAME\", \"I-PROFESSION\", \"I-LOCATION\", \"I-AGE\", \"I-DATE\", \"I-CONTACT\", \"I-ID\", \"I-PHI\",  \"O\"]\n",
    "    orig_tags = [\"NAME\", \"PROFESSION\", \"LOCATION\", \"AGE\", \"DATE\", \"CONTACT\", \"ID\", \"PHI\", \"O\"]\n",
    "    entities, tags, sub_tags = gather_tags(root)\n",
    "    \n",
    "    sentences = split_to_sent(root)\n",
    "    ents, toks = [], []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = split_to_tokens(sentence)\n",
    "        for token in tokens:\n",
    "            idx = orig_tags.index(tags[entities.index(token) if token in entities else -1])\n",
    "            ents.append(token) \n",
    "            toks.append(new_tags[idx])\n",
    "\n",
    "        ents.append(\"\")\n",
    "        toks.append(\"\")\n",
    "    \n",
    "    return ents, toks\n",
    "\n",
    "     \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nner_tags_dev = Tags()\\nfor file in listOfFileDev:\\n    tree = ET.parse(f\"./data/test/{file}\")\\n    root = tree.getroot()\\n    label_txt(root, \"dev.txt\")\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfFile = os.listdir(\"./data/NEW-PHI/\")\n",
    "listOfFileDev = listOfFile[:len(listOfFile)//2]\n",
    "listOfFileTest = listOfFile[len(listOfFile)//2:]\n",
    "\"\"\"\n",
    "ner_tags_dev = Tags()\n",
    "for file in listOfFileDev:\n",
    "    tree = ET.parse(f\"./data/test/{file}\")\n",
    "    root = tree.getroot()\n",
    "    label_txt(root, \"dev.txt\")\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0  1\n",
      "0         Record  O\n",
      "1           date  O\n",
      "2              :  O\n",
      "3     2020-05-30  O\n",
      "4             CC  O\n",
      "...          ... ..\n",
      "2156         and  O\n",
      "2157  reconciled  O\n",
      "2158        with  O\n",
      "2159     patient  O\n",
      "2160               \n",
      "\n",
      "[2161 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"ner_tags = Tags()\n",
    "for file in listOfFileTest:\n",
    "    tree = ET.parse(f\"./data/test/{file}\")\n",
    "    root = tree.getroot()\n",
    "    label_txt(root, \"test.txt\")\"\"\"\n",
    "\n",
    "listOfFile = os.listdir(\"./data/NEW-PHI/\")\n",
    "listOfFileDev = listOfFile[:len(listOfFile)//2]\n",
    "listOfFileTest = listOfFile[len(listOfFile)//2:]\n",
    "\n",
    "entities, tokens = [], []\n",
    "for file in listOfFileTest:\n",
    "    tree = ET.parse(f\"./data/NEW-PHI/{file}\")\n",
    "    root = tree.getroot()\n",
    "    ents, toks = label_txt(root)\n",
    "    entities.extend(ents)\n",
    "    tokens.extend(toks) \n",
    "\n",
    "new_tokens = [entities, tokens]\n",
    "df = pd.DataFrame(new_tokens).transpose()\n",
    "print(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"new-test.txt\", sep=\" \", index=False, columns=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
