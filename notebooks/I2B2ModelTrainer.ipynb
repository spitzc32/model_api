{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yLoZtjkgUb8",
        "outputId": "169c8c77-6d42-4498-db49-87acf7582852"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flair\n",
            "  Downloading flair-0.11.3-py3-none-any.whl (401 kB)\n",
            "\u001b[K     |████████████████████████████████| 401 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting conllu>=4.0\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2022.6.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (1.0.2)\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from flair) (9.0.0)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 60.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.10)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 49.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: gdown==4.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.4.0)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.9.1)\n",
            "Collecting transformers>=4.0.0\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 56.0 MB/s \n",
            "\u001b[?25hCollecting hyperopt>=0.2.7\n",
            "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 53.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.2)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.12.1+cu113)\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-2.0.0.tar.gz (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 63.5 MB/s \n",
            "\u001b[?25hCollecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 63.4 MB/s \n",
            "\u001b[?25hCollecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.64.1)\n",
            "Collecting pptree\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 391 kB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown==4.4.0->flair) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==4.4.0->flair) (3.8.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==4.4.0->flair) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==4.4.0->flair) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb>=0.3.2->flair) (1.21.6)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.14.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair) (1.7.3)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair) (2.6.3)\n",
            "Collecting py4j\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[K     |████████████████████████████████| 200 kB 72.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair) (0.16.0)\n",
            "Collecting requests\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2022.9.24)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (3.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
            "Building wheels for collected packages: mpld3, overrides, sqlitedict, langdetect, pptree, wikipedia-api\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=e7b2f745fa2c0faf4a2b6af06707831c6705d33b4a8f22b0583780acf2c52e60\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10188 sha256=a4c5d0e67fadc36b775f5d9e6e1508fd2e733b8db3f709fb977d938d50d64966\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.0.0-py3-none-any.whl size=15735 sha256=8f1e30288486e49b3d2b71e510b8fb2db0adbe783acbd01bceecb2b477cfa96a\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/dd/2e/0ed4a25cb73fc30c7ea8d10b50acb7226175736067e40a7ea3\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993241 sha256=d2dd764d935d40a5d62a7cc4057970f5ff25cb0c4b72d4c24899ee54cc0762ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4628 sha256=025648b4cdab65f41ec0fb155db4a8bbdaaf69ef0c1896c20aa8df1f2f7c7d7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/e8/7d/a9c3c19b4722608a0d8b05a38c36bc3f230c43becd2a46794b\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13479 sha256=ed0bafbca78124a2d637f2af2ddb3f03935d09c2623a4b374632a24d005e03a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "Successfully built mpld3 overrides sqlitedict langdetect pptree wikipedia-api\n",
            "Installing collected packages: requests, importlib-metadata, tokenizers, sentencepiece, py4j, overrides, huggingface-hub, wikipedia-api, transformers, sqlitedict, segtok, pptree, mpld3, langdetect, konoha, janome, hyperopt, ftfy, deprecated, conllu, bpemb, flair\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.13.0\n",
            "    Uninstalling importlib-metadata-4.13.0:\n",
            "      Successfully uninstalled importlib-metadata-4.13.0\n",
            "  Attempting uninstall: hyperopt\n",
            "    Found existing installation: hyperopt 0.1.2\n",
            "    Uninstalling hyperopt-0.1.2:\n",
            "      Successfully uninstalled hyperopt-0.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.4.1 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "gym 0.25.2 requires importlib-metadata>=4.8.0; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\u001b[0m\n",
            "Successfully installed bpemb-0.3.4 conllu-4.5.2 deprecated-1.2.13 flair-0.11.3 ftfy-6.1.1 huggingface-hub-0.10.1 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 mpld3-0.3 overrides-3.1.0 pptree-3.1 py4j-0.10.9.7 requests-2.28.1 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.0.0 tokenizers-0.13.2 transformers-4.24.0 wikipedia-api-0.5.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install flair\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ryy97utAgQi5"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "def get_spans_from_bio(bioes_tags, bioes_scores=None):\n",
        "    # add a dummy \"O\" to close final prediction\n",
        "    bioes_tags.append(\"O\")\n",
        "    # return complex list\n",
        "    found_spans = []\n",
        "    # internal variables\n",
        "    current_tag_weights: Dict[str, float] = defaultdict(lambda: 0.0)\n",
        "    previous_tag = \"O-\"\n",
        "    current_span = []\n",
        "    current_span_scores = []\n",
        "    for idx, bioes_tag in enumerate(bioes_tags):\n",
        "\n",
        "        # non-set tags are OUT tags\n",
        "        if bioes_tag == \"\" or bioes_tag == \"O\" or bioes_tag == \"_\":\n",
        "            bioes_tag = \"O-\"\n",
        "\n",
        "        # anything that is not OUT is IN\n",
        "        in_span = False if bioes_tag == \"O-\" else True\n",
        "\n",
        "        # does this prediction start a new span?\n",
        "        starts_new_span = False\n",
        "\n",
        "        # begin and single tags start new spans\n",
        "        if bioes_tag[0:2] in [\"B-\", \"S-\"]:\n",
        "            starts_new_span = True\n",
        "\n",
        "        # in IOB format, an I tag starts a span if it follows an O or is a different span\n",
        "        if bioes_tag[0:2] == \"I-\" and previous_tag[2:] != bioes_tag[2:]:\n",
        "            starts_new_span = True\n",
        "\n",
        "        # single tags that change prediction start new spans\n",
        "        if bioes_tag[0:2] in [\"S-\"] and previous_tag[2:] != bioes_tag[2:]:\n",
        "            starts_new_span = True\n",
        "\n",
        "        # if an existing span is ended (either by reaching O or starting a new span)\n",
        "        if (starts_new_span or not in_span) and len(current_span) > 0:\n",
        "            # determine score and value\n",
        "            span_score = sum(current_span_scores) / len(current_span_scores)\n",
        "            span_value = sorted(current_tag_weights.items(), key=lambda k_v: k_v[1], reverse=True)[0][0]\n",
        "\n",
        "            # append to result list\n",
        "            found_spans.append((current_span, span_score, span_value))\n",
        "\n",
        "            # reset for-loop variables for new span\n",
        "            current_span = []\n",
        "            current_span_scores = []\n",
        "            current_tag_weights = defaultdict(lambda: 0.0)\n",
        "\n",
        "        if in_span:\n",
        "            current_span.append(idx)\n",
        "            current_span_scores.append(bioes_scores[idx] if bioes_scores else 1.0)\n",
        "            weight = 1.1 if starts_new_span else 1.0\n",
        "            current_tag_weights[bioes_tag[2:]] += weight\n",
        "\n",
        "        # remember previous tag\n",
        "        previous_tag = bioes_tag\n",
        "\n",
        "    return found_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y0XY3bX2gz8m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import flair\n",
        "\n",
        "START_TAG: str = \"<START>\"\n",
        "STOP_TAG: str = \"<STOP>\"\n",
        "\n",
        "\n",
        "class CRF(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Conditional Random Field Implementation according to sgrvinod (https://github.com/sgrvinod).\n",
        "    Classifier which predicts single tag / class / label for given word based on not just the word,\n",
        "    but also on previous seen annotations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tag_dictionary, tagset_size: int, init_from_state_dict: bool):\n",
        "        \"\"\"\n",
        "        :param tag_dictionary: tag dictionary in order to find ID for start and stop tags\n",
        "        :param tagset_size: number of tag from tag dictionary\n",
        "        :param init_from_state_dict: whether we load pretrained model from state dict\n",
        "        \"\"\"\n",
        "        super(CRF, self).__init__()\n",
        "\n",
        "        self.tagset_size = tagset_size\n",
        "        # Transitions are used in the following way: transitions[to, from].\n",
        "        self.transitions = torch.nn.Parameter(torch.randn(tagset_size, tagset_size))\n",
        "        # If we are not using a pretrained model and train a fresh one, we need to set transitions from any tag\n",
        "        # to START-tag and from STOP-tag to any other tag to -10000.\n",
        "        if not init_from_state_dict:\n",
        "            self.transitions.detach()[tag_dictionary.get_idx_for_item(START_TAG), :] = -10000\n",
        "\n",
        "            self.transitions.detach()[:, tag_dictionary.get_idx_for_item(STOP_TAG)] = -10000\n",
        "        self.to(flair.device)\n",
        "\n",
        "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward propagation of Conditional Random Field.\n",
        "        :param features: output from RNN / Linear layer in shape (batch size, seq len, hidden size)\n",
        "        :return: CRF scores (emission scores for each token + transitions prob from previous state) in\n",
        "        shape (batch_size, seq len, tagset size, tagset size)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = features.size()[:2]\n",
        "\n",
        "        emission_scores = features\n",
        "        emission_scores = emission_scores.unsqueeze(-1).expand(batch_size, seq_len, self.tagset_size, self.tagset_size)\n",
        "\n",
        "        crf_scores = emission_scores + self.transitions.unsqueeze(0).unsqueeze(0)\n",
        "        return crf_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AftIFVXXq0Hj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import flair\n",
        "\n",
        "class LSTM(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Simple LSTM Implementation that returns the features used for (1)CRF and (2)Span Classifier\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, rnn_layers: int, hidden_size: int, bidirectional: bool, rnn_input_dim: int,):\n",
        "        \"\"\"\n",
        "        :param tag_dictionary: tag dictionary in order to find ID for start and stop tags\n",
        "        :param tagset_size: number of tag from tag dictionary\n",
        "        :param init_from_state_dict: whether we load pretrained model from state dict\n",
        "        \"\"\"\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn_input_dim = rnn_input_dim\n",
        "        self.num_layers = rnn_layers\n",
        "        self.dropout = 0.0 if rnn_layers == 1 else 0.5\n",
        "        self.bidirectional = bidirectional\n",
        "        self.batch_first = True\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            self.rnn_input_dim,\n",
        "            self.hidden_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=self.dropout,\n",
        "            bidirectional=self.bidirectional,\n",
        "            batch_first=self.batch_first,\n",
        "        )\n",
        "\n",
        "        self.to(flair.device)\n",
        "    \n",
        "    def forward(self, sentence_tensor: torch.Tensor, sorted_lengths: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward propagation of LSTM Model by packing the tensors.\n",
        "        :param features: output from RNN / Linear layer in shape (batch size, seq len, hidden size)\n",
        "        :return: CRF scores (emission scores for each token + transitions prob from previous state) in\n",
        "        shape (batch_size, seq len, tagset size, tagset size)\n",
        "        \"\"\"\n",
        "        packed = pack_padded_sequence(sentence_tensor, sorted_lengths, batch_first=True, enforce_sorted=False)\n",
        "        rnn_output, hidden = self.lstm(packed)\n",
        "        sentence_tensor, output_lengths = pad_packed_sequence(rnn_output, batch_first=True)\n",
        "\n",
        "        return sentence_tensor, output_lengths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E95ayW9IghN7"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn\n",
        "from torch.nn.functional import softmax\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "import flair\n",
        "from flair.data import Dictionary, Label, List, Sentence\n",
        "\n",
        "START_TAG: str = \"<START>\"\n",
        "STOP_TAG: str = \"<STOP>\"\n",
        "\n",
        "\n",
        "class ViterbiLoss(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Calculates the loss for each sequence up to its length t.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tag_dictionary: Dictionary):\n",
        "        \"\"\"\n",
        "        :param tag_dictionary: tag_dictionary of task\n",
        "        \"\"\"\n",
        "        super(ViterbiLoss, self).__init__()\n",
        "        self.tag_dictionary = tag_dictionary\n",
        "        self.tagset_size = len(tag_dictionary)\n",
        "        self.start_tag = tag_dictionary.get_idx_for_item(START_TAG)\n",
        "        self.stop_tag = tag_dictionary.get_idx_for_item(STOP_TAG)\n",
        "\n",
        "    def forward(self, features_tuple: tuple, targets: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward propagation of Viterbi Loss\n",
        "        :param features_tuple: CRF scores from forward method in shape (batch size, seq len, tagset size, tagset size),\n",
        "            lengths of sentences in batch, transitions from CRF\n",
        "        :param targets: true tags for sentences which will be converted to matrix indices.\n",
        "        :return: average Viterbi Loss over batch size\n",
        "        \"\"\"\n",
        "        features, lengths, transitions = features_tuple\n",
        "\n",
        "        batch_size = features.size(0)\n",
        "        seq_len = features.size(1)\n",
        "\n",
        "        targets, targets_matrix_indices = self._format_targets(targets, lengths)\n",
        "        targets_matrix_indices = torch.tensor(targets_matrix_indices, dtype=torch.long).unsqueeze(2).to(flair.device)\n",
        "\n",
        "        # scores_at_targets[range(features.shape[0]), lengths.values -1]\n",
        "        # Squeeze crf scores matrices in 1-dim shape and gather scores at targets by matrix indices\n",
        "        scores_at_targets = torch.gather(features.view(batch_size, seq_len, -1), 2, targets_matrix_indices)\n",
        "        scores_at_targets = pack_padded_sequence(scores_at_targets, lengths, batch_first=True)[0]\n",
        "        transitions_to_stop = transitions[\n",
        "            np.repeat(self.stop_tag, features.shape[0]),\n",
        "            [target[length - 1] for target, length in zip(targets, lengths)],\n",
        "        ]\n",
        "        gold_score = scores_at_targets.sum() + transitions_to_stop.sum()\n",
        "\n",
        "        scores_upto_t = torch.zeros(batch_size, self.tagset_size, device=flair.device)\n",
        "\n",
        "        for t in range(max(lengths)):\n",
        "            batch_size_t = sum(\n",
        "                [length > t for length in lengths]\n",
        "            )  # since batch is ordered, we can save computation time by reducing our effective batch_size\n",
        "\n",
        "            if t == 0:\n",
        "                # Initially, get scores from <start> tag to all other tags\n",
        "                scores_upto_t[:batch_size_t] = (\n",
        "                    scores_upto_t[:batch_size_t] + features[:batch_size_t, t, :, self.start_tag]\n",
        "                )\n",
        "            else:\n",
        "                # We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp\n",
        "                # Remember, the cur_tag of the previous timestep is the prev_tag of this timestep\n",
        "                scores_upto_t[:batch_size_t] = self._log_sum_exp(\n",
        "                    features[:batch_size_t, t, :, :] + scores_upto_t[:batch_size_t].unsqueeze(1), dim=2\n",
        "                )\n",
        "\n",
        "        all_paths_scores = self._log_sum_exp(scores_upto_t + transitions[self.stop_tag].unsqueeze(0), dim=1).sum()\n",
        "\n",
        "        viterbi_loss = all_paths_scores - gold_score\n",
        "\n",
        "        return viterbi_loss\n",
        "\n",
        "    @staticmethod\n",
        "    def _log_sum_exp(tensor, dim):\n",
        "        \"\"\"\n",
        "        Calculates the log-sum-exponent of a tensor's dimension in a numerically stable way.\n",
        "        :param tensor: tensor\n",
        "        :param dim: dimension to calculate log-sum-exp of\n",
        "        :return: log-sum-exp\n",
        "        \"\"\"\n",
        "        m, _ = torch.max(tensor, dim)\n",
        "        m_expanded = m.unsqueeze(dim).expand_as(tensor)\n",
        "        return m + torch.log(torch.sum(torch.exp(tensor - m_expanded), dim))\n",
        "\n",
        "    def _format_targets(self, targets: torch.Tensor, lengths: torch.IntTensor):\n",
        "        \"\"\"\n",
        "        Formats targets into matrix indices.\n",
        "        CRF scores contain per sentence, per token a (tagset_size x tagset_size) matrix, containing emission score for\n",
        "            token j + transition prob from previous token i. Means, if we think of our rows as \"to tag\" and our columns\n",
        "            as \"from tag\", the matrix in cell [10,5] would contain the emission score for tag 10 + transition score\n",
        "            from previous tag 5 and could directly be addressed through the 1-dim indices (10 + tagset_size * 5) = 70,\n",
        "            if our tagset consists of 12 tags.\n",
        "        :param targets: targets as in tag dictionary\n",
        "        :param lengths: lengths of sentences in batch\n",
        "        \"\"\"\n",
        "        targets_per_sentence = []\n",
        "\n",
        "        targets_list = targets.tolist()\n",
        "        for cut in lengths:\n",
        "            targets_per_sentence.append(targets_list[:cut])\n",
        "            targets_list = targets_list[cut:]\n",
        "\n",
        "        for t in targets_per_sentence:\n",
        "            t += [self.tag_dictionary.get_idx_for_item(STOP_TAG)] * (int(lengths.max().item()) - len(t))\n",
        "\n",
        "        matrix_indices = list(\n",
        "            map(\n",
        "                lambda s: [self.tag_dictionary.get_idx_for_item(START_TAG) + (s[0] * self.tagset_size)]\n",
        "                + [s[i] + (s[i + 1] * self.tagset_size) for i in range(0, len(s) - 1)],\n",
        "                targets_per_sentence,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return targets_per_sentence, matrix_indices\n",
        "\n",
        "\n",
        "class ViterbiDecoder:\n",
        "    \"\"\"\n",
        "    Decodes a given sequence using the Viterbi algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tag_dictionary: Dictionary):\n",
        "        \"\"\"\n",
        "        :param tag_dictionary: Dictionary of tags for sequence labeling task\n",
        "        \"\"\"\n",
        "        self.tag_dictionary = tag_dictionary\n",
        "        self.tagset_size = len(tag_dictionary)\n",
        "        self.start_tag = tag_dictionary.get_idx_for_item(START_TAG)\n",
        "        self.stop_tag = tag_dictionary.get_idx_for_item(STOP_TAG)\n",
        "\n",
        "    def decode(\n",
        "        self, features_tuple: tuple, probabilities_for_all_classes: bool, sentences: List[Sentence]\n",
        "    ) -> Tuple[List, List]:\n",
        "        \"\"\"\n",
        "        Decoding function returning the most likely sequence of tags.\n",
        "        :param features_tuple: CRF scores from forward method in shape (batch size, seq len, tagset size, tagset size),\n",
        "            lengths of sentence in batch, transitions of CRF\n",
        "        :param probabilities_for_all_classes: whether to return probabilities for all tags\n",
        "        :return: decoded sequences\n",
        "        \"\"\"\n",
        "        features, lengths, transitions = features_tuple\n",
        "        all_tags = []\n",
        "\n",
        "        batch_size = features.size(0)\n",
        "        seq_len = features.size(1)\n",
        "\n",
        "        # Create a tensor to hold accumulated sequence scores at each current tag\n",
        "        scores_upto_t = torch.zeros(batch_size, seq_len + 1, self.tagset_size).to(flair.device)\n",
        "        # Create a tensor to hold back-pointers\n",
        "        # i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag\n",
        "        # Let pads be the <end> tag index, since that was the last tag in the decoded sequence\n",
        "        backpointers = (\n",
        "            torch.ones((batch_size, seq_len + 1, self.tagset_size), dtype=torch.long, device=flair.device)\n",
        "            * self.stop_tag\n",
        "        )\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            batch_size_t = sum([length > t for length in lengths])  # effective batch size (sans pads) at this timestep\n",
        "            terminates = [i for i, length in enumerate(lengths) if length == t + 1]\n",
        "\n",
        "            if t == 0:\n",
        "                scores_upto_t[:batch_size_t, t] = features[:batch_size_t, t, :, self.start_tag]\n",
        "                backpointers[:batch_size_t, t, :] = (\n",
        "                    torch.ones((batch_size_t, self.tagset_size), dtype=torch.long) * self.start_tag\n",
        "                )\n",
        "            else:\n",
        "                # We add scores at current timestep to scores accumulated up to previous timestep, and\n",
        "                # choose the previous timestep that corresponds to the max. accumulated score for each current timestep\n",
        "                scores_upto_t[:batch_size_t, t], backpointers[:batch_size_t, t, :] = torch.max(\n",
        "                    features[:batch_size_t, t, :, :] + scores_upto_t[:batch_size_t, t - 1].unsqueeze(1), dim=2\n",
        "                )\n",
        "\n",
        "            # If sentence is over, add transition to STOP-tag\n",
        "            if terminates:\n",
        "                scores_upto_t[terminates, t + 1], backpointers[terminates, t + 1, :] = torch.max(\n",
        "                    scores_upto_t[terminates, t].unsqueeze(1) + transitions[self.stop_tag].unsqueeze(0), dim=2\n",
        "                )\n",
        "\n",
        "        # Decode/trace best path backwards\n",
        "        decoded = torch.zeros((batch_size, backpointers.size(1)), dtype=torch.long, device=flair.device)\n",
        "        pointer = torch.ones((batch_size, 1), dtype=torch.long, device=flair.device) * self.stop_tag\n",
        "\n",
        "        for t in list(reversed(range(backpointers.size(1)))):\n",
        "            decoded[:, t] = torch.gather(backpointers[:, t, :], 1, pointer).squeeze(1)\n",
        "            pointer = decoded[:, t].unsqueeze(1)\n",
        "\n",
        "        # Sanity check\n",
        "        assert torch.equal(\n",
        "            decoded[:, 0], torch.ones((batch_size), dtype=torch.long, device=flair.device) * self.start_tag\n",
        "        )\n",
        "\n",
        "        # remove start-tag and backscore to stop-tag\n",
        "        scores_upto_t = scores_upto_t[:, :-1, :]\n",
        "        decoded = decoded[:, 1:]\n",
        "\n",
        "        # Max + Softmax to get confidence score for predicted label and append label to each token\n",
        "        scores = softmax(scores_upto_t, dim=2)\n",
        "        confidences = torch.max(scores, dim=2)\n",
        "\n",
        "        tags = []\n",
        "        for tag_seq, tag_seq_conf, length_seq in zip(decoded, confidences.values, lengths):\n",
        "            tags.append(\n",
        "                [\n",
        "                    (self.tag_dictionary.get_item_for_index(tag), conf.item())\n",
        "                    for tag, conf in list(zip(tag_seq, tag_seq_conf))[:length_seq]\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        if probabilities_for_all_classes:\n",
        "            all_tags = self._all_scores_for_token(scores.cpu(), lengths, sentences)\n",
        "\n",
        "        return tags, all_tags\n",
        "\n",
        "    def _all_scores_for_token(self, scores: torch.Tensor, lengths: torch.IntTensor, sentences: List[Sentence]):\n",
        "        \"\"\"\n",
        "        Returns all scores for each tag in tag dictionary.\n",
        "        :param scores: Scores for current sentence.\n",
        "        \"\"\"\n",
        "        scores = scores.numpy()\n",
        "        prob_tags_per_sentence = []\n",
        "        for scores_sentence, length, sentence in zip(scores, lengths, sentences):\n",
        "            scores_sentence = scores_sentence[:length]\n",
        "            prob_tags_per_sentence.append(\n",
        "                [\n",
        "                    [\n",
        "                        Label(token, self.tag_dictionary.get_item_for_index(score_id), score)\n",
        "                        for score_id, score in enumerate(score_dist)\n",
        "                    ]\n",
        "                    for score_dist, token in zip(scores_sentence, sentence)\n",
        "                ]\n",
        "            )\n",
        "        return prob_tags_per_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4MCg4pQh7jnl"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List, Optional\n",
        "from flair.data import _PartOfSentence, DataPoint, Label\n",
        "\n",
        "class Token(_PartOfSentence):\n",
        "    \"\"\"\n",
        "    This class represents one word in a tokenized sentence. Each token may have any number of tags. It may also point\n",
        "    to its head in a dependency tree.\n",
        "\n",
        "    :param text: Single text(Token) from the sequence\n",
        "    :param head_id: the location of the text (For Document)\n",
        "    :param whitespace_after: if token has whitespace\n",
        "    :param start_position: what character number in document does this token start?\n",
        "    :param sentence: If token belongs to sentence, indicate here which var it belongs to\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        text: str,\n",
        "        head_id: int = None,\n",
        "        whitespace_after: int = 1,\n",
        "        start_position: int = 0,\n",
        "        sentence=None,\n",
        "    ):\n",
        "        super().__init__(sentence=sentence)\n",
        "\n",
        "        self.form: str = text\n",
        "        self._internal_index: Optional[int] = None\n",
        "        self.head_id: Optional[int] = head_id\n",
        "        self.whitespace_after: int = whitespace_after\n",
        "\n",
        "        self.start_pos = start_position\n",
        "        self.end_pos = start_position + len(text)\n",
        "\n",
        "        self._embeddings: Dict = {}\n",
        "        self.tags_proba_dist: Dict[str, List[Label]] = {}\n",
        "\n",
        "    @property\n",
        "    def idx(self) -> int:\n",
        "        if isinstance(self._internal_index, int):\n",
        "            return self._internal_index\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "    @property\n",
        "    def text(self):\n",
        "        return self.form\n",
        "\n",
        "    @property\n",
        "    def unlabeled_identifier(self) -> str:\n",
        "        return f'Token[{self.idx-1}]: \"{self.text}\"'\n",
        "\n",
        "    def add_tags_proba_dist(self, tag_type: str, tags: List[Label]):\n",
        "        self.tags_proba_dist[tag_type] = tags\n",
        "\n",
        "    def get_tags_proba_dist(self, tag_type: str) -> List[Label]:\n",
        "        if tag_type in self.tags_proba_dist:\n",
        "            return self.tags_proba_dist[tag_type]\n",
        "        return []\n",
        "\n",
        "    def get_head(self):\n",
        "        return self.sentence.get_token(self.head_id)\n",
        "\n",
        "    @property\n",
        "    def start_position(self) -> int:\n",
        "        return self.start_pos\n",
        "\n",
        "    @property\n",
        "    def end_position(self) -> int:\n",
        "        return self.end_pos\n",
        "\n",
        "    @property\n",
        "    def embedding(self):\n",
        "        return self.get_embedding()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__str__()\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.form.split(\" \"))\n",
        "\n",
        "    def add_label(self, typename: str, value: str, score: float = 1.0):\n",
        "        \"\"\"\n",
        "        The Token is a special _PartOfSentence in that it may be initialized without a Sentence.\n",
        "        Therefore, labels get added only to the Sentence if it exists\n",
        "        \"\"\"\n",
        "        if self.sentence:\n",
        "            super().add_label(typename=typename, value=value, score=score)\n",
        "        else:\n",
        "            DataPoint.add_label(self, typename=typename, value=value, score=score)\n",
        "\n",
        "    def set_label(self, typename: str, value: str, score: float = 1.0):\n",
        "        \"\"\"\n",
        "        The Token is a special _PartOfSentence in that it may be initialized without a Sentence.\n",
        "        Therefore, labels get set only to the Sentence if it exists\n",
        "        \"\"\"\n",
        "        if self.sentence:\n",
        "            super().set_label(typename=typename, value=value, score=score)\n",
        "        else:\n",
        "            DataPoint.set_label(self, typename=typename, value=value, score=score)\n",
        "\n",
        "\n",
        "class Span(_PartOfSentence):\n",
        "    \"\"\"\n",
        "    This class represents one textual span consisting of Tokens. It may be used for the instance that the \n",
        "    tokens form in a nested nature, meaning the tokens combined together forms a long phrase.\n",
        "\n",
        "    :param tokens: List of tokens in the span\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokens: List[Token]):\n",
        "        super().__init__(tokens[0].sentence)\n",
        "        self.tokens = tokens\n",
        "        super()._init_labels()\n",
        "\n",
        "    @property\n",
        "    def start_position(self) -> int:\n",
        "        return self.tokens[0].start_position\n",
        "\n",
        "    @property\n",
        "    def end_position(self) -> int:\n",
        "        return self.tokens[-1].end_position\n",
        "\n",
        "    @property\n",
        "    def text(self) -> str:\n",
        "        return \" \".join([t.text for t in self.tokens])\n",
        "\n",
        "    @property\n",
        "    def unlabeled_identifier(self) -> str:\n",
        "        return f'Span[{self.tokens[0].idx -1}:{self.tokens[-1].idx}]: \"{self.text}\"'\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__str__()\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Token:\n",
        "        return self.tokens[idx]\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.tokens)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.tokens)\n",
        "\n",
        "    @property\n",
        "    def embedding(self):\n",
        "        pass\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class LockedDropout(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of locked (or variational) dropout. \n",
        "    Randomly drops out entire parameters in embedding space.\n",
        "\n",
        "    :param dropout_rate: represent the fraction of the input unit to be dropped. It will be from 0 to 1.\n",
        "    :param batch_first: represent if the drop will perform in an ascending manner\n",
        "    :param inplace: \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dropout_rate=0.5, batch_first=True, inplace=False):\n",
        "        super(LockedDropout, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.batch_first = batch_first\n",
        "        self.inplace = inplace\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training or not self.dropout_rate:\n",
        "            return x\n",
        "\n",
        "        if not self.batch_first:\n",
        "            m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - self.dropout_rate)\n",
        "        else:\n",
        "            m = x.data.new(x.size(0), 1, x.size(2)).bernoulli_(1 - self.dropout_rate)\n",
        "\n",
        "        mask = torch.autograd.Variable(m, requires_grad=False) / (1 - self.dropout_rate)\n",
        "        mask = mask.expand_as(x)\n",
        "        return mask * x\n",
        "\n",
        "    def extra_repr(self):\n",
        "        inplace_str = \", inplace\" if self.inplace else \"\"\n",
        "        return \"p={}{}\".format(self.dropout_rate, inplace_str)\n",
        "\n",
        "\n",
        "class WordDropout(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of word dropout. Randomly drops out entire words \n",
        "    (or characters) in embedding space.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dropout_rate=0.05, inplace=False):\n",
        "        super(WordDropout, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.inplace = inplace\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training or not self.dropout_rate:\n",
        "            return x\n",
        "\n",
        "        m = x.data.new(x.size(0), x.size(1), 1).bernoulli_(1 - self.dropout_rate)\n",
        "\n",
        "        mask = torch.autograd.Variable(m, requires_grad=False)\n",
        "        return mask * x\n",
        "\n",
        "    def extra_repr(self):\n",
        "        inplace_str = \", inplace\" if self.inplace else \"\"\n",
        "        return \"p={}{}\".format(self.dropout_rate, inplace_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zAjoyg7Vg99a"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "from urllib.error import HTTPError\n",
        "\n",
        "import torch\n",
        "import torch.nn \n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from tqdm import tqdm\n",
        "\n",
        "import flair.nn\n",
        "from flair.data import Dictionary, Sentence\n",
        "from flair.datasets import DataLoader, FlairDatapointDataset\n",
        "from flair.embeddings import StackedEmbeddings, TokenEmbeddings\n",
        "from flair.file_utils import cached_path\n",
        "from flair.training_utils import store_embeddings\n",
        "\n",
        "log = logging.getLogger(\"flair\")\n",
        "\n",
        "\n",
        "class Bi_LSTM_CRF(flair.nn.Classifier[Sentence]):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embeddings: TokenEmbeddings,\n",
        "        tag_dictionary: Dictionary,\n",
        "        tag_type: str,\n",
        "        rnn: Optional[torch.nn.RNN] = None,\n",
        "        tag_format: str = \"BIOES\",\n",
        "        hidden_size: int = 256,\n",
        "        rnn_layers: int = 1,\n",
        "        bidirectional: bool = True,\n",
        "        use_crf: bool = True,\n",
        "        ave_embeddings: bool = True,\n",
        "        dropout: float = 0.0,\n",
        "        word_dropout: float = 0.05,\n",
        "        locked_dropout: float = 0.5,\n",
        "        loss_weights: Dict[str, float] = None,\n",
        "        init_from_state_dict: bool = False,\n",
        "        allow_unk_predictions: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        BiLSTM Span CRF class for predicting labels for single tokens. Can be parameterized by several attributes.\n",
        "        Span prediction is utilized if there are nested entities such as Address and Organization. Since the researchers\n",
        "        observed that the token are have different length for a given dataset, we made the Span useful by incorporating it \n",
        "        only if the data needs it. \n",
        "\n",
        "        :param embeddings: Embeddings to use during training and prediction\n",
        "        :param tag_dictionary: Dictionary containing all tags from corpus which can be predicted\n",
        "        :param tag_type: type of tag which is going to be predicted in case a corpus has multiple annotations\n",
        "        :param rnn: (Optional) Takes a torch.nn.Module as parameter by which you can pass a shared RNN between\n",
        "            different tasks.\n",
        "        :param hidden_size: Hidden size of RNN layer\n",
        "        :param rnn_layers: number of RNN layers\n",
        "        :param bidirectional: If True, RNN becomes bidirectional\n",
        "        :param use_crf: If True, use a Conditional Random Field for prediction, else linear map to tag space.\n",
        "        :param ave_embeddings: If True, add a linear layer on top of embeddings, if you want to imitate\n",
        "            fine tune non-trainable embeddings.\n",
        "        :param dropout: If > 0, then use dropout.\n",
        "        :param word_dropout: If > 0, then use word dropout.\n",
        "        :param locked_dropout: If > 0, then use locked dropout.\n",
        "        :param loss_weights: Dictionary of weights for labels for the loss function\n",
        "            (if any label's weight is unspecified it will default to 1.0)\n",
        "        :param init_from_state_dict: Indicator whether we are loading a model from state dict\n",
        "            since we need to transform previous models' weights into CRF instance weights\n",
        "        \"\"\"\n",
        "        super(Bi_LSTM_CRF, self).__init__()\n",
        "\n",
        "        # ----- Create the internal tag dictionary -----\n",
        "        self.tag_type = tag_type\n",
        "        self.tag_format = tag_format.upper()\n",
        "        if init_from_state_dict:\n",
        "            self.label_dictionary = tag_dictionary\n",
        "        else:\n",
        "            # span-labels need special encoding (BIO or BIOES)\n",
        "            if tag_dictionary.span_labels:\n",
        "                # the big question is whether the label dictionary should contain an UNK or not\n",
        "                # without UNK, we cannot evaluate on data that contains labels not seen in test\n",
        "                # with UNK, the model learns less well if there are no UNK examples\n",
        "                self.label_dictionary = Dictionary(add_unk=allow_unk_predictions)\n",
        "                assert self.tag_format in [\"BIOES\", \"BIO\"]\n",
        "                for label in tag_dictionary.get_items():\n",
        "                    if label == \"<unk>\":\n",
        "                        continue\n",
        "                    self.label_dictionary.add_item(\"O\")\n",
        "                    if self.tag_format == \"BIOES\":\n",
        "                        self.label_dictionary.add_item(\"S-\" + label)\n",
        "                        self.label_dictionary.add_item(\"B-\" + label)\n",
        "                        self.label_dictionary.add_item(\"E-\" + label)\n",
        "                        self.label_dictionary.add_item(\"I-\" + label)\n",
        "                    if self.tag_format == \"BIO\":\n",
        "                        self.label_dictionary.add_item(\"B-\" + label)\n",
        "                        self.label_dictionary.add_item(\"I-\" + label)\n",
        "            else:\n",
        "                self.label_dictionary = tag_dictionary\n",
        "\n",
        "        # is this a span prediction problem?\n",
        "        self.predict_spans = self._determine_if_span_prediction_problem(self.label_dictionary)\n",
        "\n",
        "        self.tagset_size = len(self.label_dictionary)\n",
        "        log.info(f\"SequenceTagger predicts: {self.label_dictionary}\")\n",
        "\n",
        "        # ----- Embeddings -----\n",
        "        # We set the first initial embeddings gathered from Flair \n",
        "        # Stacked and concatenated then ave. using Linear\n",
        "        self.embeddings = embeddings\n",
        "        embedding_dim: int = embeddings.embedding_length\n",
        "\n",
        "        # ----- Initial loss weights parameters -----\n",
        "        # This is for reiteration process of training.\n",
        "        # Initially we don't have any loss weights, but as we proceed to training, \n",
        "        # we get loss computations from the evaluation stage.\n",
        "        self.weight_dict = loss_weights\n",
        "        self.loss_weights = self._init_loss_weights(loss_weights) if loss_weights else None\n",
        "\n",
        "        # ----- RNN specific parameters -----\n",
        "        # These parameters are for setting up the self.RNN \n",
        "        self.hidden_size = hidden_size if not rnn else rnn.hidden_size\n",
        "        self.rnn_layers = rnn_layers if not rnn else rnn.num_layers\n",
        "        self.bidirectional = bidirectional if not rnn else rnn.bidirectional\n",
        "\n",
        "        # ----- Conditional Random Field parameters -----\n",
        "        self.use_crf = use_crf\n",
        "        # Previously trained models have been trained without an explicit CRF, thus it is required to check\n",
        "        # whether we are loading a model from state dict in order to skip or add START and STOP token\n",
        "        if use_crf and not init_from_state_dict and not self.label_dictionary.start_stop_tags_are_set():\n",
        "            self.label_dictionary.set_start_stop_tags()\n",
        "            self.tagset_size += 2\n",
        "\n",
        "        # ----- Dropout parameters -----\n",
        "        # dropouts\n",
        "        self.use_dropout: float = dropout\n",
        "        self.use_word_dropout: float = word_dropout\n",
        "        self.use_locked_dropout: float = locked_dropout\n",
        "\n",
        "        if dropout > 0.0:\n",
        "            self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        if word_dropout > 0.0:\n",
        "            self.word_dropout = flair.nn.WordDropout(word_dropout)\n",
        "\n",
        "        if locked_dropout > 0.0:\n",
        "            self.locked_dropout = flair.nn.LockedDropout(locked_dropout)\n",
        "\n",
        "        # ----- Model layers -----\n",
        "        # Initialize Embedding Linear Dim for the purpose of ave them\n",
        "        self.ave_embeddings = ave_embeddings\n",
        "        if self.ave_embeddings:\n",
        "            self.embedding2nn = torch.nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "        # ----- RNN layer -----\n",
        "        # If shared RNN provided, else create one for model\n",
        "        self.rnn: torch.nn.RNN = (\n",
        "            rnn\n",
        "            if rnn\n",
        "            else LSTM(\n",
        "                rnn_layers,\n",
        "                hidden_size,\n",
        "                bidirectional,\n",
        "                rnn_input_dim=embedding_dim,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        num_directions = 2 if self.bidirectional else 1\n",
        "        hidden_output_dim = self.rnn.hidden_size * num_directions\n",
        "\n",
        "     \n",
        "        # final linear map to tag space\n",
        "        self.linear = torch.nn.Linear(hidden_output_dim, len(self.label_dictionary))\n",
        "\n",
        "\n",
        "        # the loss function is Viterbi if using CRF, else regular Cross Entropy Loss\n",
        "        self.loss_function = (\n",
        "            ViterbiLoss(self.label_dictionary)\n",
        "        )\n",
        "\n",
        "        # if using CRF, we also require a CRF and a Viterbi decoder\n",
        "        if use_crf:\n",
        "            self.crf = CRF(self.label_dictionary, self.tagset_size, init_from_state_dict)\n",
        "            self.viterbi_decoder = ViterbiDecoder(self.label_dictionary)\n",
        "\n",
        "        self.to(flair.device)\n",
        "\n",
        "    @property\n",
        "    def label_type(self):\n",
        "        return self.tag_type\n",
        "\n",
        "    def _init_loss_weights(self, loss_weights: Dict[str, float]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Intializes the loss weights based on given dictionary:\n",
        "        :param loss_weights: dictionary - contains loss weights\n",
        "        \"\"\"\n",
        "        n_classes = len(self.label_dictionary)\n",
        "        weight_list = [1.0 for _ in range(n_classes)]\n",
        "        for i, tag in enumerate(self.label_dictionary.get_items()):\n",
        "            if tag in loss_weights.keys():\n",
        "                weight_list[i] = loss_weights[tag]\n",
        "\n",
        "        return torch.tensor(weight_list).to(flair.device)\n",
        "\n",
        "    def forward_loss(self, sentences: Union[List[Sentence], Sentence]) -> Tuple[torch.Tensor, int]:\n",
        "        \"\"\"\n",
        "        Calculates the loss of the forward propagation of the model\n",
        "        :param sentences: either a listof sentence or just a sentence\n",
        "        \"\"\"\n",
        "        # if there are no sentences, there is no loss\n",
        "        if len(sentences) == 0:\n",
        "            return torch.tensor(0.0, dtype=torch.float, device=flair.device, requires_grad=True), 0\n",
        "\n",
        "        # forward pass to get scores\n",
        "        scores, gold_labels = self.forward(sentences)  # type: ignore\n",
        "\n",
        "        # calculate loss given scores and labels\n",
        "        return self._calculate_loss(scores, gold_labels)\n",
        "\n",
        "    def forward(self, sentences: Union[List[Sentence], Sentence]):\n",
        "        \"\"\"\n",
        "        Forward propagation through network. Returns gold labels of batch in addition.\n",
        "        :param sentences: Batch of current sentences\n",
        "        \"\"\"\n",
        "        if not isinstance(sentences, list):\n",
        "            sentences = [sentences]\n",
        "        self.embeddings.embed(sentences)\n",
        "\n",
        "        # make a zero-padded tensor for the whole sentence\n",
        "        lengths, sentence_tensor = self._make_padded_tensor_for_batch(sentences)\n",
        "\n",
        "        # sort tensor in decreasing order based on lengths of sentences in batch\n",
        "        sorted_lengths, length_indices = lengths.sort(dim=0, descending=True)\n",
        "        sentences = [sentences[i] for i in length_indices]\n",
        "        sentence_tensor = sentence_tensor[length_indices]\n",
        "\n",
        "        # ----- Forward Propagation -----\n",
        "        # we get the dropout we initialize for th regularization\n",
        "        # of our inputs\n",
        "        if self.use_dropout:\n",
        "            sentence_tensor = self.dropout(sentence_tensor)\n",
        "        if self.use_word_dropout:\n",
        "            sentence_tensor = self.word_dropout(sentence_tensor)\n",
        "        if self.use_locked_dropout:\n",
        "            sentence_tensor = self.locked_dropout(sentence_tensor)\n",
        "\n",
        "        # Average the embeddings using Linear Transform\n",
        "        if self.ave_embeddings:\n",
        "            sentence_tensor = self.embedding2nn(sentence_tensor)\n",
        "\n",
        "        # This packs our Sentence tensor form, the process for weighting\n",
        "        # our LSTM model\n",
        "        sentence_tensor, output_lengths = self.rnn(sentence_tensor, sorted_lengths)\n",
        "\n",
        "        # Regularize our computed sentence tensor form the LSTM model\n",
        "        if self.use_dropout:\n",
        "            sentence_tensor = self.dropout(sentence_tensor)\n",
        "        if self.use_locked_dropout:\n",
        "            sentence_tensor = self.locked_dropout(sentence_tensor)\n",
        "\n",
        "        # linear map to tag space\n",
        "        features = self.linear(sentence_tensor)\n",
        "\n",
        "        # Depending on whether we are using CRF or a linear layer, scores is either:\n",
        "        # -- A tensor of shape (batch size, sequence length, tagset size, tagset size) for CRF\n",
        "        # -- A tensor of shape (aggregated sequence length for all sentences in batch, tagset size) for linear layer\n",
        "        if self.use_crf:\n",
        "            features = self.crf(features)\n",
        "            scores = (features, sorted_lengths, self.crf.transitions)\n",
        "        else:\n",
        "            scores = self._get_scores_from_features(features, sorted_lengths)\n",
        "\n",
        "        # get the gold labels\n",
        "        gold_labels = self._get_gold_labels(sentences)\n",
        "\n",
        "        return scores, gold_labels\n",
        "\n",
        "    def _calculate_loss(self, scores, labels) -> Tuple[torch.Tensor, int]:\n",
        "\n",
        "        if not any(labels):\n",
        "            return torch.tensor(0.0, requires_grad=True, device=flair.device), 1\n",
        "\n",
        "        labels = torch.tensor(\n",
        "            [\n",
        "                self.label_dictionary.get_idx_for_item(label[0])\n",
        "                if len(label) > 0\n",
        "                else self.label_dictionary.get_idx_for_item(\"O\")\n",
        "                for label in labels\n",
        "            ],\n",
        "            dtype=torch.long,\n",
        "            device=flair.device,\n",
        "        )\n",
        "\n",
        "        return self.loss_function(scores, labels), len(labels)\n",
        "\n",
        "    def _make_padded_tensor_for_batch(self, sentences: List[Sentence]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        makes zero padded tensors in the shape of the max longest sentence and the embedding_length to match \n",
        "        the shape of the embedding in feeding to our LSTM model.\n",
        "        :param sentences: Batch of current sentences\n",
        "        \"\"\"\n",
        "        names = self.embeddings.get_names()\n",
        "        tok_lengths: List[int] = [len(sentence.tokens) for sentence in sentences]\n",
        "        longest_token_sequence_in_batch: int = max(tok_lengths)\n",
        "        zero_tensor = torch.zeros(\n",
        "            self.embeddings.embedding_length * longest_token_sequence_in_batch,\n",
        "            dtype=torch.float,\n",
        "            device=flair.device,\n",
        "        )\n",
        "        all_embs = list()\n",
        "        for sentence in sentences:\n",
        "            all_embs += [emb for token in sentence for emb in token.get_each_embedding(names)]\n",
        "            nb_padding_tokens = longest_token_sequence_in_batch - len(sentence)\n",
        "\n",
        "            if nb_padding_tokens > 0:\n",
        "                t = zero_tensor[: self.embeddings.embedding_length * nb_padding_tokens]\n",
        "                all_embs.append(t)\n",
        "\n",
        "        sentence_tensor = torch.cat(all_embs).view(\n",
        "            [\n",
        "                len(sentences),\n",
        "                longest_token_sequence_in_batch,\n",
        "                self.embeddings.embedding_length,\n",
        "            ]\n",
        "        )\n",
        "        return torch.tensor(tok_lengths, dtype=torch.long), sentence_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_scores_from_features(features: torch.Tensor, lengths: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Trims current batch tensor in shape (batch size, sequence length, tagset size) in such a way that all\n",
        "        pads are going to be removed.\n",
        "        :param features: torch.tensor containing all features from forward propagation\n",
        "        :param lengths: length from each sentence in batch in order to trim padding tokens\n",
        "        \"\"\"\n",
        "        features_formatted = []\n",
        "        for feat, lens in zip(features, lengths):\n",
        "            features_formatted.append(feat[:lens])\n",
        "        scores = torch.cat(features_formatted)\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def _get_gold_labels(self, sentences: Union[List[Sentence], Sentence]):\n",
        "        \"\"\"\n",
        "        Extracts gold labels from each sentence.\n",
        "        :param sentences: List of sentences in batch\n",
        "        \"\"\"\n",
        "        # spans need to be encoded as token-level predictions\n",
        "        if self.predict_spans:\n",
        "            all_sentence_labels = []\n",
        "            for sentence in sentences:\n",
        "                sentence_labels = [\"O\"] * len(sentence)\n",
        "                for label in sentence.get_labels(self.label_type):\n",
        "                    span: Span = label.data_point #if type(label.data_point) == Span else [label.data_point]\n",
        "                    if self.tag_format == \"BIOES\":\n",
        "                        if len(span) == 1:\n",
        "                            sentence_labels[span[0].idx - 1] = \"S-\" + label.value\n",
        "                        else:\n",
        "                            sentence_labels[span[0].idx - 1] = \"B-\" + label.value\n",
        "                            sentence_labels[span[-1].idx - 1] = \"E-\" + label.value\n",
        "                            for i in range(span[0].idx, span[-1].idx - 1):\n",
        "                                sentence_labels[i] = \"I-\" + label.value\n",
        "                    else:\n",
        "                        sentence_labels[span[0].idx - 1] = \"B-\" + label.value\n",
        "                        for i in range(span[0].idx, span[-1].idx):\n",
        "                            sentence_labels[i] = \"I-\" + label.value\n",
        "                all_sentence_labels.extend(sentence_labels)\n",
        "            labels = [[label] for label in all_sentence_labels]\n",
        "\n",
        "        # all others are regular labels for each token\n",
        "        else:\n",
        "            labels = [[token.get_label(self.label_type, \"O\").value] for sentence in sentences for token in sentence]\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        sentences: Union[List[Sentence], Sentence],\n",
        "        mini_batch_size: int = 32,\n",
        "        return_probabilities_for_all_classes: bool = False,\n",
        "        verbose: bool = False,\n",
        "        label_name: Optional[str] = None,\n",
        "        return_loss=False,\n",
        "        embedding_storage_mode=\"none\",\n",
        "        force_token_predictions: bool = False,\n",
        "    ):  # type: ignore\n",
        "        \"\"\"\n",
        "        Predicts labels for current batch with CRF.\n",
        "        :param sentences: List of sentences in batch\n",
        "        :param mini_batch_size: batch size for test data\n",
        "        :param return_probabilities_for_all_classes: Whether to return probabilites for all classes\n",
        "        :param verbose: whether to use progress bar\n",
        "        :param label_name: which label to predict\n",
        "        :param return_loss: whether to return loss value\n",
        "        :param embedding_storage_mode: determines where to store embeddings - can be \"gpu\", \"cpu\" or None.\n",
        "        \"\"\"\n",
        "        if label_name is None:\n",
        "            label_name = self.tag_type\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if not sentences:\n",
        "                return sentences\n",
        "\n",
        "            # make sure its a list\n",
        "            if not isinstance(sentences, list) and not isinstance(sentences, flair.data.Dataset):\n",
        "                sentences = [sentences]\n",
        "\n",
        "            # filter empty sentences\n",
        "            sentences = [sentence for sentence in sentences if len(sentence) > 0]\n",
        "\n",
        "            # reverse sort all sequences by their length\n",
        "            reordered_sentences = sorted(sentences, key=lambda s: len(s), reverse=True)\n",
        "\n",
        "            if len(reordered_sentences) == 0:\n",
        "                return sentences\n",
        "\n",
        "            dataloader = DataLoader(\n",
        "                dataset=FlairDatapointDataset(reordered_sentences),\n",
        "                batch_size=mini_batch_size,\n",
        "            )\n",
        "            # progress bar for verbosity\n",
        "            if verbose:\n",
        "                dataloader = tqdm(dataloader, desc=\"Batch inference\")\n",
        "\n",
        "            overall_loss = torch.zeros(1, device=flair.device)\n",
        "            batch_no = 0\n",
        "            label_count = 0\n",
        "            for batch in dataloader:\n",
        "\n",
        "                batch_no += 1\n",
        "\n",
        "                # stop if all sentences are empty\n",
        "                if not batch:\n",
        "                    continue\n",
        "\n",
        "                # get features from forward propagation\n",
        "                features, gold_labels = self.forward(batch)\n",
        "\n",
        "                # remove previously predicted labels of this type\n",
        "                for sentence in batch:\n",
        "                    sentence.remove_labels(label_name)\n",
        "\n",
        "                # if return_loss, get loss value\n",
        "                if return_loss:\n",
        "                    loss = self._calculate_loss(features, gold_labels)\n",
        "                    overall_loss += loss[0]\n",
        "                    label_count += loss[1]\n",
        "\n",
        "                # Sort batch in same way as forward propagation\n",
        "                lengths = torch.LongTensor([len(sentence) for sentence in batch])\n",
        "                _, sort_indices = lengths.sort(dim=0, descending=True)\n",
        "                batch = [batch[i] for i in sort_indices]\n",
        "\n",
        "                # make predictions\n",
        "                if self.use_crf:\n",
        "                    predictions, all_tags = self.viterbi_decoder.decode(\n",
        "                        features, return_probabilities_for_all_classes, batch\n",
        "                    )\n",
        "                else:\n",
        "                    predictions, all_tags = self._standard_inference(\n",
        "                        features, batch, return_probabilities_for_all_classes\n",
        "                    )\n",
        "\n",
        "                # add predictions to Sentence\n",
        "                for sentence, sentence_predictions in zip(batch, predictions):\n",
        "\n",
        "                    # BIOES-labels need to be converted to spans\n",
        "                    if self.predict_spans and not force_token_predictions:\n",
        "                        sentence_tags = [label[0] for label in sentence_predictions]\n",
        "                        sentence_scores = [label[1] for label in sentence_predictions]\n",
        "                        predicted_spans = get_spans_from_bio(sentence_tags, sentence_scores)\n",
        "                        for predicted_span in predicted_spans:\n",
        "                            span: Span = sentence[predicted_span[0][0] : predicted_span[0][-1] + 1]\n",
        "                            span.add_label(label_name, value=predicted_span[2], score=predicted_span[1])\n",
        "\n",
        "                    # token-labels can be added directly (\"O\" and legacy \"_\" predictions are skipped)\n",
        "                    else:\n",
        "                        for token, label in zip(sentence.tokens, sentence_predictions):\n",
        "                            if label[0] in [\"O\", \"_\"]:\n",
        "                                continue\n",
        "                            token.add_label(typename=label_name, value=label[0], score=label[1])\n",
        "\n",
        "                # all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided\n",
        "                for (sentence, sent_all_tags) in zip(batch, all_tags):\n",
        "                    for (token, token_all_tags) in zip(sentence.tokens, sent_all_tags):\n",
        "                        token.add_tags_proba_dist(label_name, token_all_tags)\n",
        "\n",
        "                store_embeddings(sentences, storage_mode=embedding_storage_mode)\n",
        "\n",
        "            if return_loss:\n",
        "                return overall_loss, label_count\n",
        "\n",
        "    def _standard_inference(self, features: torch.Tensor, batch: List[Sentence], probabilities_for_all_classes: bool):\n",
        "        \"\"\"\n",
        "        Softmax over emission scores from forward propagation.\n",
        "        :param features: sentence tensor from forward propagation\n",
        "        :param batch: list of sentence\n",
        "        :param probabilities_for_all_classes: whether to return score for each tag in tag dictionary\n",
        "        \"\"\"\n",
        "        softmax_batch = F.softmax(features, dim=1).cpu()\n",
        "        scores_batch, prediction_batch = torch.max(softmax_batch, dim=1)\n",
        "        predictions = []\n",
        "        all_tags = []\n",
        "\n",
        "        for sentence in batch:\n",
        "            scores = scores_batch[: len(sentence)]\n",
        "            predictions_for_sentence = prediction_batch[: len(sentence)]\n",
        "            predictions.append(\n",
        "                [\n",
        "                    (self.label_dictionary.get_item_for_index(prediction), score.item())\n",
        "                    for token, score, prediction in zip(sentence, scores, predictions_for_sentence)\n",
        "                ]\n",
        "            )\n",
        "            scores_batch = scores_batch[len(sentence) :]\n",
        "            prediction_batch = prediction_batch[len(sentence) :]\n",
        "\n",
        "        if probabilities_for_all_classes:\n",
        "            lengths = [len(sentence) for sentence in batch]\n",
        "            all_tags = self._all_scores_for_token(batch, softmax_batch, lengths)\n",
        "\n",
        "        return predictions, all_tags\n",
        "\n",
        "    def _all_scores_for_token(self, sentences: List[Sentence], scores: torch.Tensor, lengths: List[int]):\n",
        "        \"\"\"\n",
        "        Returns all scores for each tag in tag dictionary.\n",
        "        :param scores: Scores for current sentence.\n",
        "        \"\"\"\n",
        "        scores = scores.numpy()\n",
        "        tokens = [token for sentence in sentences for token in sentence]\n",
        "        prob_all_tags = [\n",
        "            [\n",
        "                Label(token, self.label_dictionary.get_item_for_index(score_id), score)\n",
        "                for score_id, score in enumerate(score_dist)\n",
        "            ]\n",
        "            for score_dist, token in zip(scores, tokens)\n",
        "        ]\n",
        "\n",
        "        prob_tags_per_sentence = []\n",
        "        previous = 0\n",
        "        for length in lengths:\n",
        "            prob_tags_per_sentence.append(prob_all_tags[previous : previous + length])\n",
        "            previous = length\n",
        "        return prob_tags_per_sentence\n",
        "\n",
        "    def _get_state_dict(self):\n",
        "        \"\"\"Returns the state dictionary for this model.\"\"\"\n",
        "        model_state = {\n",
        "            **super()._get_state_dict(),\n",
        "            \"embeddings\": self.embeddings,\n",
        "            \"hidden_size\": self.hidden_size,\n",
        "            \"tag_dictionary\": self.label_dictionary,\n",
        "            \"tag_format\": self.tag_format,\n",
        "            \"tag_type\": self.tag_type,\n",
        "            \"use_crf\": self.use_crf,\n",
        "            \"rnn_layers\": self.rnn_layers,\n",
        "            \"use_dropout\": self.use_dropout,\n",
        "            \"use_word_dropout\": self.use_word_dropout,\n",
        "            \"use_locked_dropout\": self.use_locked_dropout,\n",
        "            \"ave_embeddings\": self.ave_embeddings,\n",
        "            \"weight_dict\": self.weight_dict,\n",
        "        }\n",
        "\n",
        "        return model_state\n",
        "\n",
        "    @classmethod\n",
        "    def _init_model_with_state_dict(cls, state, **kwargs):\n",
        "\n",
        "        if state[\"use_crf\"]:\n",
        "            if \"transitions\" in state[\"state_dict\"]:\n",
        "                state[\"state_dict\"][\"crf.transitions\"] = state[\"state_dict\"][\"transitions\"]\n",
        "                del state[\"state_dict\"][\"transitions\"]\n",
        "\n",
        "        return super()._init_model_with_state_dict(\n",
        "            state,\n",
        "            embeddings=state.get(\"embeddings\"),\n",
        "            tag_dictionary=state.get(\"tag_dictionary\"),\n",
        "            tag_format=state.get(\"tag_format\", \"BIOES\"),\n",
        "            tag_type=state.get(\"tag_type\"),\n",
        "            use_crf=state.get(\"use_crf\"),\n",
        "            rnn_layers=state.get(\"rnn_layers\"),\n",
        "            hidden_size=state.get(\"hidden_size\"),\n",
        "            dropout=state.get(\"use_dropout\", 0.0),\n",
        "            word_dropout=state.get(\"use_word_dropout\", 0.0),\n",
        "            locked_dropout=state.get(\"use_locked_dropout\", 0.0),\n",
        "            ave_embeddings=state.get(\"ave_embeddings\", True),\n",
        "            loss_weights=state.get(\"weight_dict\"),\n",
        "            init_from_state_dict=True,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _filter_empty_sentences(sentences: List[Sentence]) -> List[Sentence]:\n",
        "        filtered_sentences = [sentence for sentence in sentences if sentence.tokens]\n",
        "        if len(sentences) != len(filtered_sentences):\n",
        "            log.warning(f\"Ignore {len(sentences) - len(filtered_sentences)} sentence(s) with no tokens.\")\n",
        "        return filtered_sentences\n",
        "\n",
        "    def _determine_if_span_prediction_problem(self, dictionary: Dictionary) -> bool:\n",
        "        for item in dictionary.get_items():\n",
        "            if item.startswith(\"B-\") or item.startswith(\"S-\") or item.startswith(\"I-\"):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def _print_predictions(self, batch, gold_label_type):\n",
        "\n",
        "        lines = []\n",
        "        if self.predict_spans:\n",
        "            for datapoint in batch:\n",
        "                # all labels default to \"O\"\n",
        "                for token in datapoint:\n",
        "                    token.set_label(\"gold_bio\", \"O\")\n",
        "                    token.set_label(\"predicted_bio\", \"O\")\n",
        "\n",
        "                # set gold token-level\n",
        "                for gold_label in datapoint.get_labels(gold_label_type):\n",
        "                    gold_span: Span = gold_label.data_point\n",
        "                    prefix = \"B-\"\n",
        "                    for token in gold_span:\n",
        "                        token.set_label(\"gold_bio\", prefix + gold_label.value)\n",
        "                        prefix = \"I-\"\n",
        "\n",
        "                # set predicted token-level\n",
        "                for predicted_label in datapoint.get_labels(\"predicted\"):\n",
        "                    predicted_span: Span = predicted_label.data_point\n",
        "                    prefix = \"B-\"\n",
        "                    for token in predicted_span:\n",
        "                        token.set_label(\"predicted_bio\", prefix + predicted_label.value)\n",
        "                        prefix = \"I-\"\n",
        "\n",
        "                # now print labels in CoNLL format\n",
        "                for token in datapoint:\n",
        "                    eval_line = (\n",
        "                        f\"{token.text} \"\n",
        "                        f\"{token.get_label('gold_bio').value} \"\n",
        "                        f\"{token.get_label('predicted_bio').value}\\n\"\n",
        "                    )\n",
        "                    lines.append(eval_line)\n",
        "                lines.append(\"\\n\")\n",
        "\n",
        "        else:\n",
        "            for datapoint in batch:\n",
        "                # print labels in CoNLL format\n",
        "                for token in datapoint:\n",
        "                    eval_line = (\n",
        "                        f\"{token.text} \"\n",
        "                        f\"{token.get_label(gold_label_type).value} \"\n",
        "                        f\"{token.get_label('predicted').value}\\n\"\n",
        "                    )\n",
        "                    lines.append(eval_line)\n",
        "                lines.append(\"\\n\")\n",
        "        return lines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268,
          "referenced_widgets": [
            "e679b0bc6f3c4e27b3e6d3da3f87760a",
            "bff3fc9908364702ac498d2b1b877197",
            "01e8854f83134311ad6737cb0be12d6a",
            "7284a40c44034d268f59bb681970cb52",
            "e38c0aeb51c4447a860a7199a83861c6",
            "704fce40b5f14acd8a027d84d860a952",
            "47d67416b8744999936ce2bc44136124",
            "cb356434a7ad4cc892e78035a10e1518",
            "701128250e42464692932fa7eaf6f932",
            "e2a57b3ee9d14ab0a28f7ecf796bc09b",
            "e7ca817928f54a7eafcdc27e680e253c",
            "a0152673633f4afead8998de56c2940c",
            "4970538cd4804577ab1f887b87536b6d",
            "cd551b000819498f9747d33c30ee91c7",
            "36fb28b1ea3b44acb03af5f47384d5c6",
            "59a3dc7b6d204a64964de10de3127a41",
            "cf446915217f4c0ab0b6e3433d196ecc",
            "ce846118ccd34bc59194bb167eb51e97",
            "9e6e3600c7894856ba44c0966807f1da",
            "05a3b44a85c94814b6966cc1e265b44b",
            "a7c20a4cd14b48bfaaf16e011bf2492e",
            "ebefe617062d476e82fdae84b520efe1",
            "24ff3b22d8c446699fa566bafd683306",
            "57bb183a88c4411b850f05df51ba32a4",
            "b7c63503465e4626aa5c0dee128e9720",
            "f4672214d3cd4e888f834cde1814d01f",
            "32f880ba7bd54eaba8771edd8de07ca4",
            "3b5c288ea37b4b2dbc63bf3d198bde26",
            "e16a78534fd743f2bba7e438d78782ef",
            "6eb17b264be24f2f8874f78b9c58da10",
            "a5eaa644982c4b17aaf774d40c616b57",
            "51a151972b81416bad7b9c332e49fbf1",
            "50adce73bea24df0af1ff567b7653e47",
            "ddd7f479c5c44534a2d79ff6746c1ae5",
            "57fafb7b6de3438aa53e4a8a6cff649f",
            "91395d1bc6fe48c9bc7815c7c4cefd7b",
            "7c420db2819945db865ef7d30eb9c1dc",
            "57e01845f01b4a35b6731f36675e76f8",
            "09828a98af4f4307a1a816c23ea8c89b",
            "c20f07612a594f62a374fb208b322ee9",
            "601ede14ed5e4bcdb2cd88a884f195fe",
            "a9eef7dcff7b42b983426c294936da01",
            "79c2ca7131d94aa2a6d7d1d20f5b3dd6",
            "4992ef9654b44e73bc5d42447692b277",
            "5c848945aec0409eacb885ec3772eee6",
            "65409d969c3f47599949a60e2957edee",
            "7c7ba49472f14f1ebad0516feb50c7dc",
            "680e68e448154ff195941e62b8785151",
            "eb1a519cefd24f369f71654a1aa4c403",
            "9ede617a43c94f8e8d28f19111040286",
            "80872f5be75447bf9de3a5c1009d05c6",
            "ce4ca82671f745f6933fdf7c693fb386",
            "729845a3b3924c9abb947e675ad043c8",
            "09e61b562394477a9dbad173e76f52f6",
            "b0de112553964e13b7ca778152f94d96"
          ]
        },
        "id": "O3gyoxxmjXBL",
        "outputId": "3234c149-630f-4444-ddfa-4be750a173b9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e679b0bc6f3c4e27b3e6d3da3f87760a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0152673633f4afead8998de56c2940c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24ff3b22d8c446699fa566bafd683306",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddd7f479c5c44534a2d79ff6746c1ae5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c848945aec0409eacb885ec3772eee6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-14 03:20:40,836 https://flair.informatik.hu-berlin.de/resources/characters/common_characters not found in cache, downloading to /tmp/tmp846h_t0c\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2887/2887 [00:00<00:00, 720899.90B/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-14 03:20:41,379 copying /tmp/tmp846h_t0c to cache at /root/.flair/datasets/common_characters\n",
            "2022-11-14 03:20:41,383 removing temp file /tmp/tmp846h_t0c\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from flair.embeddings import (\n",
        "    TransformerWordEmbeddings,\n",
        "    WordEmbeddings,\n",
        "    FlairEmbeddings,\n",
        "    CharacterEmbeddings,\n",
        "    DocumentRNNEmbeddings,\n",
        "    OneHotEmbeddings,\n",
        "    StackedEmbeddings\n",
        ")\n",
        "from flair.data import Sentence\n",
        "import torch\n",
        "import flair\n",
        "\n",
        "flair.device = torch.device('cuda:0') \n",
        "\n",
        "#flair_forward_embedding = FlairEmbeddings(\"flair/ner-english-large\")\n",
        "#flair_backward_embedding = FlairEmbeddings(\"news-backward-fast\")\n",
        "\n",
        "bert_embedding = TransformerWordEmbeddings(model='bert-base-uncased',\n",
        "                                       fine_tune=True,\n",
        "                                       use_context=True,\n",
        "                                       )\n",
        "char_embedding = CharacterEmbeddings()\n",
        "\n",
        "embeddings = bert_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZodc3YykMlM",
        "outputId": "791d6f8e-fc81-4cd6-8e90-ea10b0701f4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "2022-11-14 03:21:01,006 Reading data from /content\n",
            "2022-11-14 03:21:01,012 Train: /content/train.txt\n",
            "2022-11-14 03:21:01,017 Dev: /content/dev.txt\n",
            "2022-11-14 03:21:01,020 Test: /content/test.txt\n",
            "33557\n",
            "2022-11-14 03:21:16,979 Computing label dictionary. Progress:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "33557it [00:00, 53336.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-14 03:21:17,620 Dictionary created for label 'ner' with 9 values: DATE (seen 7190 times), NAME (seen 4291 times), LOCATION (seen 2405 times), AGE (seen 827 times), ID (seen 629 times), PROFESSION (seen 472 times), CONTACT (seen 331 times), PHI (seen 5 times)\n",
            "Dictionary with 9 tags: <unk>, DATE, NAME, LOCATION, AGE, ID, PROFESSION, CONTACT, PHI\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from flair.data import Corpus\n",
        "from flair.datasets import ColumnCorpus\n",
        "columns = {0 : 'text', 1:'ner'}\n",
        "# directory where the data resides\n",
        "data_folder = '/content/'\n",
        "# initializing the corpus\n",
        "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
        "                              train_file = '/content/train.txt',\n",
        "                              test_file = '/content/test.txt',\n",
        "                              dev_file = '/content/dev.txt')\n",
        "print(len(corpus.train))\n",
        "\n",
        "# tag to predict\n",
        "tag_type = 'ner'\n",
        "# make tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_label_dictionary(label_type=tag_type)\n",
        "print(tag_dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sUCC6xYft60"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "PBQoOTLBhGlG",
        "outputId": "9d6deecd-4329-43d5-cfd0-169e2effcc47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-14 03:21:17,640 SequenceTagger predicts: Dictionary with 33 tags: O, S-DATE, B-DATE, E-DATE, I-DATE, S-NAME, B-NAME, E-NAME, I-NAME, S-LOCATION, B-LOCATION, E-LOCATION, I-LOCATION, S-AGE, B-AGE, E-AGE, I-AGE, S-ID, B-ID, E-ID, I-ID, S-PROFESSION, B-PROFESSION, E-PROFESSION, I-PROFESSION, S-CONTACT, B-CONTACT, E-CONTACT, I-CONTACT, S-PHI, B-PHI, E-PHI, I-PHI\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# 7. start training\\ntrainer.train('/content/drive/MyDrive/bertner/finalPHI-flert-5e3',\\n              learning_rate=5e-3,\\n              embeddings_storage_mode='none',\\n              mini_batch_size=4,\\n              max_epochs=50,\\n              write_weights = True,\\n              checkpoint=True)\\n\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from flair.trainers import ModelTrainer\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "# 5. initialize sequence tagger\n",
        "tagger = Bi_LSTM_CRF(hidden_size=256,\n",
        "                        embeddings=embeddings,\n",
        "                        tag_dictionary=tag_dictionary,\n",
        "                        #train_with_dev=True,\n",
        "                        dropout=0.5,\n",
        "                        tag_type=tag_type,\n",
        "                        use_crf=True,\n",
        "                        )\n",
        "\n",
        "# 6. initialize trainer\n",
        "trainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "\"\"\"\n",
        "# 7. start training\n",
        "trainer.train('/content/drive/MyDrive/bertner/finalPHI-flert-5e3',\n",
        "              learning_rate=5e-3,\n",
        "              embeddings_storage_mode='none',\n",
        "              mini_batch_size=4,\n",
        "              max_epochs=50,\n",
        "              write_weights = True,\n",
        "              checkpoint=True)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXs-GvpQWoPw",
        "outputId": "d15a9545-6016-4c47-808c-f44b63dfe036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-14 03:21:17,689 loading file /content/drive/MyDrive/bertner/finalPHI-flert-5e3-resume4/checkpoint.pt\n",
            "2022-11-14 03:21:24,023 SequenceTagger predicts: Dictionary with 35 tags: O, S-DATE, B-DATE, E-DATE, I-DATE, S-NAME, B-NAME, E-NAME, I-NAME, S-LOCATION, B-LOCATION, E-LOCATION, I-LOCATION, S-AGE, B-AGE, E-AGE, I-AGE, S-ID, B-ID, E-ID, I-ID, S-PROFESSION, B-PROFESSION, E-PROFESSION, I-PROFESSION, S-CONTACT, B-CONTACT, E-CONTACT, I-CONTACT, S-PHI, B-PHI, E-PHI, I-PHI, <START>, <STOP>\n",
            "2022-11-14 03:21:24,248 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-14 03:21:24,254 Model: \"Bi_LSTM_CRF(\n",
            "  (embeddings): TransformerWordEmbeddings(\n",
            "    (model): BertModel(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (rnn): LSTM(\n",
            "    (lstm): LSTM(768, 256, batch_first=True, bidirectional=True)\n",
            "  )\n",
            "  (linear): Linear(in_features=512, out_features=35, bias=True)\n",
            "  (loss_function): ViterbiLoss()\n",
            "  (crf): CRF()\n",
            ")\"\n",
            "2022-11-14 03:21:24,258 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-14 03:21:24,264 Corpus: \"Corpus: 33557 train + 11419 dev + 11100 test sentences\"\n",
            "2022-11-14 03:21:24,267 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-14 03:21:24,268 Parameters:\n",
            "2022-11-14 03:21:24,270  - learning_rate: \"0.005000\"\n",
            "2022-11-14 03:21:24,272  - mini_batch_size: \"4\"\n",
            "2022-11-14 03:21:24,273  - patience: \"3\"\n",
            "2022-11-14 03:21:24,275  - anneal_factor: \"0.5\"\n",
            "2022-11-14 03:21:24,276  - max_epochs: \"110\"\n",
            "2022-11-14 03:21:24,277  - shuffle: \"True\"\n",
            "2022-11-14 03:21:24,278  - train_with_dev: \"False\"\n",
            "2022-11-14 03:21:24,280  - batch_growth_annealing: \"False\"\n",
            "2022-11-14 03:21:24,281 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-14 03:21:24,283 Model training base path: \"/content/drive/MyDrive/bertner/finalPHI-flert-5e3-resume5\"\n",
            "2022-11-14 03:21:24,284 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-14 03:21:24,285 Device: cuda:0\n",
            "2022-11-14 03:21:24,286 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-14 03:21:24,287 Embeddings storage mode: none\n",
            "2022-11-14 03:21:24,289 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-14 03:24:39,130 epoch 23 - iter 839/8390 - loss 0.00914726 - samples/sec: 17.23 - lr: 0.005000\n",
            "2022-11-14 03:28:04,618 epoch 23 - iter 1678/8390 - loss 0.01217670 - samples/sec: 16.91 - lr: 0.005000\n",
            "2022-11-14 03:31:24,867 epoch 23 - iter 2517/8390 - loss 0.01178384 - samples/sec: 17.21 - lr: 0.005000\n",
            "2022-11-14 03:34:46,479 epoch 23 - iter 3356/8390 - loss 0.01136096 - samples/sec: 17.12 - lr: 0.005000\n",
            "2022-11-14 03:38:04,581 epoch 23 - iter 4195/8390 - loss 0.01117552 - samples/sec: 17.41 - lr: 0.005000\n",
            "2022-11-14 03:41:24,757 epoch 23 - iter 5034/8390 - loss 0.01117956 - samples/sec: 17.22 - lr: 0.005000\n",
            "2022-11-14 03:44:46,130 epoch 23 - iter 5873/8390 - loss 0.01165258 - samples/sec: 17.11 - lr: 0.005000\n",
            "2022-11-14 03:48:02,935 epoch 23 - iter 6712/8390 - loss 0.01152376 - samples/sec: 17.54 - lr: 0.005000\n",
            "2022-11-14 03:51:29,302 epoch 23 - iter 7551/8390 - loss 0.01184589 - samples/sec: 16.69 - lr: 0.005000\n",
            "2022-11-14 03:54:46,556 epoch 23 - iter 8390/8390 - loss 0.01199524 - samples/sec: 17.48 - lr: 0.005000\n",
            "2022-11-14 03:54:51,975 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-14 03:54:51,979 EPOCH 23 done: loss 0.0120 - lr 0.005000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2855/2855 [06:22<00:00,  7.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-14 04:01:14,566 Evaluating as a multi-label problem: False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-14 04:01:14,653 DEV : loss 0.07067502290010452 - f1-score (micro avg)  0.9399\n",
            "2022-11-14 04:01:14,875 BAD EPOCHS (no improvement): 0\n",
            "2022-11-14 04:01:16,628 saving best model\n",
            "2022-11-14 04:01:18,322 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-14 04:04:37,821 epoch 24 - iter 839/8390 - loss 0.01006023 - samples/sec: 16.83 - lr: 0.005000\n",
            "2022-11-14 04:08:01,137 epoch 24 - iter 1678/8390 - loss 0.01305179 - samples/sec: 17.01 - lr: 0.005000\n",
            "2022-11-14 04:11:23,281 epoch 24 - iter 2517/8390 - loss 0.01281387 - samples/sec: 17.15 - lr: 0.005000\n",
            "2022-11-14 04:14:41,479 epoch 24 - iter 3356/8390 - loss 0.01161490 - samples/sec: 17.47 - lr: 0.005000\n",
            "2022-11-14 04:18:05,163 epoch 24 - iter 4195/8390 - loss 0.01195209 - samples/sec: 16.98 - lr: 0.005000\n",
            "2022-11-14 04:21:24,348 epoch 24 - iter 5034/8390 - loss 0.01202359 - samples/sec: 17.37 - lr: 0.005000\n",
            "2022-11-14 04:24:45,318 epoch 24 - iter 5873/8390 - loss 0.01216609 - samples/sec: 17.15 - lr: 0.005000\n",
            "2022-11-14 04:28:03,759 epoch 24 - iter 6712/8390 - loss 0.01242021 - samples/sec: 17.38 - lr: 0.005000\n",
            "2022-11-14 04:31:23,214 epoch 24 - iter 7551/8390 - loss 0.01229139 - samples/sec: 17.28 - lr: 0.005000\n",
            "2022-11-14 04:34:39,326 epoch 24 - iter 8390/8390 - loss 0.01239087 - samples/sec: 17.60 - lr: 0.005000\n",
            "2022-11-14 04:34:44,715 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-14 04:34:44,722 EPOCH 24 done: loss 0.0124 - lr 0.005000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2855/2855 [06:21<00:00,  7.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-14 04:41:06,269 Evaluating as a multi-label problem: False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-14 04:41:06,328 DEV : loss 0.06791391223669052 - f1-score (micro avg)  0.9504\n",
            "2022-11-14 04:41:06,564 BAD EPOCHS (no improvement): 0\n",
            "2022-11-14 04:41:08,302 saving best model\n",
            "2022-11-14 04:41:10,045 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-14 04:44:31,252 epoch 25 - iter 839/8390 - loss 0.01178038 - samples/sec: 16.69 - lr: 0.002500\n",
            "2022-11-14 04:47:51,112 epoch 25 - iter 1678/8390 - loss 0.01076129 - samples/sec: 17.25 - lr: 0.002500\n",
            "2022-11-14 04:51:18,039 epoch 25 - iter 2517/8390 - loss 0.01097379 - samples/sec: 16.64 - lr: 0.002500\n",
            "2022-11-14 04:54:38,310 epoch 25 - iter 3356/8390 - loss 0.01115176 - samples/sec: 17.21 - lr: 0.002500\n",
            "2022-11-14 04:57:52,353 epoch 25 - iter 4195/8390 - loss 0.01046997 - samples/sec: 17.78 - lr: 0.002500\n",
            "2022-11-14 05:01:09,490 epoch 25 - iter 5034/8390 - loss 0.01022361 - samples/sec: 17.49 - lr: 0.002500\n",
            "2022-11-14 05:04:36,519 epoch 25 - iter 5873/8390 - loss 0.01036567 - samples/sec: 16.63 - lr: 0.002500\n",
            "2022-11-14 05:07:53,699 epoch 25 - iter 6712/8390 - loss 0.01052811 - samples/sec: 17.53 - lr: 0.002500\n",
            "2022-11-14 05:11:10,542 epoch 25 - iter 7551/8390 - loss 0.01032528 - samples/sec: 17.51 - lr: 0.002500\n",
            "2022-11-14 05:14:31,084 epoch 25 - iter 8390/8390 - loss 0.01060672 - samples/sec: 17.19 - lr: 0.002500\n",
            "2022-11-14 05:14:36,825 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-14 05:14:36,830 EPOCH 25 done: loss 0.0106 - lr 0.005000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2855/2855 [06:24<00:00,  7.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-14 05:21:01,609 Evaluating as a multi-label problem: False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-14 05:21:01,672 DEV : loss 0.06811513006687164 - f1-score (micro avg)  0.9557\n",
            "2022-11-14 05:21:01,904 BAD EPOCHS (no improvement): 1\n",
            "2022-11-14 05:21:03,465 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-14 05:24:20,049 epoch 26 - iter 839/8390 - loss 0.01123101 - samples/sec: 17.08 - lr: 0.002500\n",
            "2022-11-14 05:27:36,541 epoch 26 - iter 1678/8390 - loss 0.00990390 - samples/sec: 17.55 - lr: 0.002500\n",
            "2022-11-14 05:30:54,860 epoch 26 - iter 2517/8390 - loss 0.01093459 - samples/sec: 17.41 - lr: 0.002500\n",
            "2022-11-14 05:34:13,528 epoch 26 - iter 3356/8390 - loss 0.01064690 - samples/sec: 17.35 - lr: 0.002500\n",
            "2022-11-14 05:37:28,716 epoch 26 - iter 4195/8390 - loss 0.01045820 - samples/sec: 17.67 - lr: 0.002500\n",
            "2022-11-14 05:40:50,897 epoch 26 - iter 5034/8390 - loss 0.00988700 - samples/sec: 17.04 - lr: 0.002500\n",
            "2022-11-14 05:44:06,433 epoch 26 - iter 5873/8390 - loss 0.00968713 - samples/sec: 17.67 - lr: 0.002500\n",
            "2022-11-14 05:47:26,174 epoch 26 - iter 6712/8390 - loss 0.00968024 - samples/sec: 17.25 - lr: 0.002500\n",
            "2022-11-14 05:50:44,917 epoch 26 - iter 7551/8390 - loss 0.00985157 - samples/sec: 17.33 - lr: 0.002500\n",
            "2022-11-14 05:54:02,561 epoch 26 - iter 8390/8390 - loss 0.00970579 - samples/sec: 17.44 - lr: 0.002500\n",
            "2022-11-14 05:54:07,842 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-14 05:54:07,843 EPOCH 26 done: loss 0.0097 - lr 0.005000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 13/2855 [00:01<05:38,  8.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-14 05:54:09,405 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-14 05:54:09,412 Exiting from training early.\n",
            "2022-11-14 05:54:09,414 Saving model ...\n",
            "2022-11-14 05:54:10,926 Done.\n",
            "2022-11-14 05:54:10,933 ----------------------------------------------------------------------------------------------------\n",
            "2022-11-14 05:54:10,939 loading file /content/drive/MyDrive/bertner/finalPHI-flert-5e3-resume5/best-model.pt\n",
            "2022-11-14 05:54:13,695 SequenceTagger predicts: Dictionary with 35 tags: O, S-DATE, B-DATE, E-DATE, I-DATE, S-NAME, B-NAME, E-NAME, I-NAME, S-LOCATION, B-LOCATION, E-LOCATION, I-LOCATION, S-AGE, B-AGE, E-AGE, I-AGE, S-ID, B-ID, E-ID, I-ID, S-PROFESSION, B-PROFESSION, E-PROFESSION, I-PROFESSION, S-CONTACT, B-CONTACT, E-CONTACT, I-CONTACT, S-PHI, B-PHI, E-PHI, I-PHI, <START>, <STOP>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2775/2775 [06:12<00:00,  7.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-14 06:00:26,466 Evaluating as a multi-label problem: False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-14 06:00:26,522 0.9604\t0.9510\t0.9557\t0.9454\n",
            "2022-11-14 06:00:26,525 \n",
            "Results:\n",
            "- F-score (micro) 0.9557\n",
            "- F-score (macro) 0.9454\n",
            "- Accuracy 0.9554\n",
            "\n",
            "By class:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        DATE     0.9691    0.9646    0.9668      2233\n",
            "        NAME     0.9547    0.9517    0.9532      1323\n",
            "    LOCATION     0.9457    0.9460    0.9458       722\n",
            "         AGE     0.9613    0.9408    0.9509       252\n",
            "  PROFESSION     0.8810    0.9497    0.9141       199\n",
            "          ID     0.9648    0.9506    0.9576       203\n",
            "     CONTACT     0.8983    0.9400    0.9187       106\n",
            "\n",
            "   micro avg     0.9604    0.9510    0.9557      5038\n",
            "   macro avg     0.9393    0.9490    0.9439      5038\n",
            "weighted avg     0.9329    0.9492    0.9425      5038\n",
            "\n",
            "2022-11-14 06:00:26,528 ----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "path= '/content/drive/MyDrive/bertner/finalPHI-flert-5e3-resume'\n",
        "trained_model = Bi_LSTM_CRF.load(path + '4/checkpoint.pt')\n",
        "\n",
        "# resume training best model, but this time until epoch 25 (Changed 09/03/2022)\n",
        "trainer.resume(trained_model,\n",
        "               base_path=path + '5',\n",
        "               max_epochs=110,\n",
        "               )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "WVnXZqiGHLnP",
        "outputId": "07824c1e-9f2e-4894-deb0-afe4e8fbfdf3"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mephemeral\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       readonly=readonly)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 125\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sw6r87GweZyl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01e8854f83134311ad6737cb0be12d6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb356434a7ad4cc892e78035a10e1518",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_701128250e42464692932fa7eaf6f932",
            "value": 28
          }
        },
        "05a3b44a85c94814b6966cc1e265b44b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09828a98af4f4307a1a816c23ea8c89b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09e61b562394477a9dbad173e76f52f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24ff3b22d8c446699fa566bafd683306": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_57bb183a88c4411b850f05df51ba32a4",
              "IPY_MODEL_b7c63503465e4626aa5c0dee128e9720",
              "IPY_MODEL_f4672214d3cd4e888f834cde1814d01f"
            ],
            "layout": "IPY_MODEL_32f880ba7bd54eaba8771edd8de07ca4"
          }
        },
        "32f880ba7bd54eaba8771edd8de07ca4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36fb28b1ea3b44acb03af5f47384d5c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7c20a4cd14b48bfaaf16e011bf2492e",
            "placeholder": "​",
            "style": "IPY_MODEL_ebefe617062d476e82fdae84b520efe1",
            "value": " 570/570 [00:00&lt;00:00, 19.0kB/s]"
          }
        },
        "3b5c288ea37b4b2dbc63bf3d198bde26": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47d67416b8744999936ce2bc44136124": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4970538cd4804577ab1f887b87536b6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf446915217f4c0ab0b6e3433d196ecc",
            "placeholder": "​",
            "style": "IPY_MODEL_ce846118ccd34bc59194bb167eb51e97",
            "value": "Downloading: 100%"
          }
        },
        "4992ef9654b44e73bc5d42447692b277": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50adce73bea24df0af1ff567b7653e47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51a151972b81416bad7b9c332e49fbf1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57bb183a88c4411b850f05df51ba32a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b5c288ea37b4b2dbc63bf3d198bde26",
            "placeholder": "​",
            "style": "IPY_MODEL_e16a78534fd743f2bba7e438d78782ef",
            "value": "Downloading: 100%"
          }
        },
        "57e01845f01b4a35b6731f36675e76f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57fafb7b6de3438aa53e4a8a6cff649f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09828a98af4f4307a1a816c23ea8c89b",
            "placeholder": "​",
            "style": "IPY_MODEL_c20f07612a594f62a374fb208b322ee9",
            "value": "Downloading: 100%"
          }
        },
        "59a3dc7b6d204a64964de10de3127a41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c848945aec0409eacb885ec3772eee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_65409d969c3f47599949a60e2957edee",
              "IPY_MODEL_7c7ba49472f14f1ebad0516feb50c7dc",
              "IPY_MODEL_680e68e448154ff195941e62b8785151"
            ],
            "layout": "IPY_MODEL_eb1a519cefd24f369f71654a1aa4c403"
          }
        },
        "601ede14ed5e4bcdb2cd88a884f195fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65409d969c3f47599949a60e2957edee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ede617a43c94f8e8d28f19111040286",
            "placeholder": "​",
            "style": "IPY_MODEL_80872f5be75447bf9de3a5c1009d05c6",
            "value": "Downloading: 100%"
          }
        },
        "680e68e448154ff195941e62b8785151": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09e61b562394477a9dbad173e76f52f6",
            "placeholder": "​",
            "style": "IPY_MODEL_b0de112553964e13b7ca778152f94d96",
            "value": " 440M/440M [00:16&lt;00:00, 34.7MB/s]"
          }
        },
        "6eb17b264be24f2f8874f78b9c58da10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "701128250e42464692932fa7eaf6f932": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "704fce40b5f14acd8a027d84d860a952": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7284a40c44034d268f59bb681970cb52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2a57b3ee9d14ab0a28f7ecf796bc09b",
            "placeholder": "​",
            "style": "IPY_MODEL_e7ca817928f54a7eafcdc27e680e253c",
            "value": " 28.0/28.0 [00:00&lt;00:00, 332B/s]"
          }
        },
        "729845a3b3924c9abb947e675ad043c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79c2ca7131d94aa2a6d7d1d20f5b3dd6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c420db2819945db865ef7d30eb9c1dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79c2ca7131d94aa2a6d7d1d20f5b3dd6",
            "placeholder": "​",
            "style": "IPY_MODEL_4992ef9654b44e73bc5d42447692b277",
            "value": " 466k/466k [00:00&lt;00:00, 793kB/s]"
          }
        },
        "7c7ba49472f14f1ebad0516feb50c7dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce4ca82671f745f6933fdf7c693fb386",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_729845a3b3924c9abb947e675ad043c8",
            "value": 440473133
          }
        },
        "80872f5be75447bf9de3a5c1009d05c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91395d1bc6fe48c9bc7815c7c4cefd7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_601ede14ed5e4bcdb2cd88a884f195fe",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9eef7dcff7b42b983426c294936da01",
            "value": 466062
          }
        },
        "9e6e3600c7894856ba44c0966807f1da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ede617a43c94f8e8d28f19111040286": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0152673633f4afead8998de56c2940c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4970538cd4804577ab1f887b87536b6d",
              "IPY_MODEL_cd551b000819498f9747d33c30ee91c7",
              "IPY_MODEL_36fb28b1ea3b44acb03af5f47384d5c6"
            ],
            "layout": "IPY_MODEL_59a3dc7b6d204a64964de10de3127a41"
          }
        },
        "a5eaa644982c4b17aaf774d40c616b57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7c20a4cd14b48bfaaf16e011bf2492e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9eef7dcff7b42b983426c294936da01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b0de112553964e13b7ca778152f94d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7c63503465e4626aa5c0dee128e9720": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6eb17b264be24f2f8874f78b9c58da10",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5eaa644982c4b17aaf774d40c616b57",
            "value": 231508
          }
        },
        "bff3fc9908364702ac498d2b1b877197": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_704fce40b5f14acd8a027d84d860a952",
            "placeholder": "​",
            "style": "IPY_MODEL_47d67416b8744999936ce2bc44136124",
            "value": "Downloading: 100%"
          }
        },
        "c20f07612a594f62a374fb208b322ee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb356434a7ad4cc892e78035a10e1518": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd551b000819498f9747d33c30ee91c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e6e3600c7894856ba44c0966807f1da",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05a3b44a85c94814b6966cc1e265b44b",
            "value": 570
          }
        },
        "ce4ca82671f745f6933fdf7c693fb386": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce846118ccd34bc59194bb167eb51e97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf446915217f4c0ab0b6e3433d196ecc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddd7f479c5c44534a2d79ff6746c1ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_57fafb7b6de3438aa53e4a8a6cff649f",
              "IPY_MODEL_91395d1bc6fe48c9bc7815c7c4cefd7b",
              "IPY_MODEL_7c420db2819945db865ef7d30eb9c1dc"
            ],
            "layout": "IPY_MODEL_57e01845f01b4a35b6731f36675e76f8"
          }
        },
        "e16a78534fd743f2bba7e438d78782ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2a57b3ee9d14ab0a28f7ecf796bc09b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e38c0aeb51c4447a860a7199a83861c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e679b0bc6f3c4e27b3e6d3da3f87760a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bff3fc9908364702ac498d2b1b877197",
              "IPY_MODEL_01e8854f83134311ad6737cb0be12d6a",
              "IPY_MODEL_7284a40c44034d268f59bb681970cb52"
            ],
            "layout": "IPY_MODEL_e38c0aeb51c4447a860a7199a83861c6"
          }
        },
        "e7ca817928f54a7eafcdc27e680e253c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb1a519cefd24f369f71654a1aa4c403": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebefe617062d476e82fdae84b520efe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4672214d3cd4e888f834cde1814d01f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51a151972b81416bad7b9c332e49fbf1",
            "placeholder": "​",
            "style": "IPY_MODEL_50adce73bea24df0af1ff567b7653e47",
            "value": " 232k/232k [00:00&lt;00:00, 746kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
